{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f2344b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ecceb237",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = [\"SLHC\",\"HST\",\"HCT\",\"MCV\",\"MCH\",\"MCHC\",\"RDWCV\",\"SLTC\",\"SLBC\"]\n",
    "numerical_columns2 = [\"FERRITIN\",\"FE\"]    \n",
    "model1_name = '../models/model_9_features.pkl'\n",
    "model2_name = '../models/model_2_features.pkl'\n",
    "\n",
    "\n",
    "numerical_columns = [\"SLHC\",\"HST\",\"HCT\",\"MCV\",\"MCH\",\"MCHC\",\"RDWCV\",\"SLTC\",\"SLBC\"]\n",
    "numerical_columns2 = [\"FERRITIN\",\"FE\", \"HBA1\",\"HBA2\"]  \n",
    "model1_name = '../models/model_9_features.pkl'\n",
    "model2_name = '../models/model_4_features.pkl'\n",
    "\n",
    "numerical_columns = [\"SLHC\",\"HST\",\"HCT\",\"MCV\",\"MCH\",\"MCHC\",\"RDWCV\",\"SLTC\",\"SLBC\",\"FERRITIN\",\"FE\"]\n",
    "numerical_columns2 = [\"HBA1\",\"HBA2\"]   \n",
    "model1_name = '../models/model_11_features.pkl'\n",
    "model2_name = '../models/model_hba.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a18861b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_main_data(file_path = '../datasets/splited_train_test.csv'):\n",
    "    global numerical_columns\n",
    "\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "    df.drop_duplicates(subset=['PID'], keep='last', inplace=True)\n",
    "#     df = df[~(df.sheet.isin(['d1']) & (df.thalas == 0))]\n",
    "    \n",
    "    num_df = df[numerical_columns].apply(pd.to_numeric, errors='coerce', downcast='float')\n",
    "    \n",
    "    df = df.reindex(num_df.dropna().index)\n",
    "    df[numerical_columns] = num_df.dropna()\n",
    "\n",
    "    test_df = df[df.train==0]\n",
    "    train_df = df[df.train==1]\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_model(model_name):\n",
    "    with open(model_name, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "model1 = load_model(model1_name)\n",
    "model2 = load_model(model2_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "855e3d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------\n",
      "evaluation results :\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.79      0.85      1000\n",
      "         1.0       0.82      0.93      0.87      1000\n",
      "\n",
      "    accuracy                           0.86      2000\n",
      "   macro avg       0.87      0.86      0.86      2000\n",
      "weighted avg       0.87      0.86      0.86      2000\n",
      "\n",
      "---------------------------------------------------\n",
      "226\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.66      0.79      1000\n",
      "         1.0       0.74      0.99      0.85      1000\n",
      "\n",
      "    accuracy                           0.82      2000\n",
      "   macro avg       0.87      0.82      0.82      2000\n",
      "weighted avg       0.87      0.82      0.82      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def evaluate(model1, model2, df, desc=''):\n",
    "    threshold = 0.6\n",
    "    X = df[numerical_columns]\n",
    "    y = df['thalas']\n",
    "    preds1 = np.array(model1.predict(X))\n",
    "    preds1 = np.where(preds1 >= threshold, 1, 0) # dự đoán model 1\n",
    "    print('---------------------------------------------------')\n",
    "    print(f\"evaluation results {desc}:\\n\")\n",
    "    print(classification_report(y, preds1))\n",
    "    print('---------------------------------------------------')\n",
    "    # mask = preds != y\n",
    "    mask = (preds1 == 0) & ((X.MCV < 85) | (X.MCH < 28))\n",
    "    X2 = df[numerical_columns2][mask] # lấy những sample khả năng bị dự đoán sai ra để đưa vào mô hình 2\n",
    "    print(mask.sum())\n",
    "    preds2 = np.array(model2.predict(X2))\n",
    "    preds2 = np.where(preds2 >= threshold, 1, 0)\n",
    "    y1 = y[~mask]\n",
    "    y2 = y[mask]\n",
    "    y3 = np.concatenate([y1, y2])\n",
    "    print(classification_report(y3, np.concatenate([preds1[~mask], preds2])))\n",
    "\n",
    "_, test_df = read_main_data()\n",
    "evaluate(model1, model2, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1158a3e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2000.000000\n",
       "mean       78.788414\n",
       "std        10.956099\n",
       "min        44.099998\n",
       "25%        69.475000\n",
       "50%        79.900002\n",
       "75%        88.300003\n",
       "max       113.099998\n",
       "Name: MCV, dtype: float64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.MCV.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9890aa38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
